{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Module: prune.py\n",
    "    Author: Matthew McGonagle\n",
    "    \n",
    "    Classes and functions to do cost-complexity pruning on a sci-kit learn DecisionTreeClassifier. As described in Hastie, Tibshirani, and Friedman's\n",
    "    \"The Elements of Statistical Learning\", one method to prevent overfitting of a decision tree classifier is to grow the tree very far, and then\n",
    "    prune it back in such a way that we add in a weighted cost for the size of the tree. This method allows different branches of the tree to\n",
    "    grow to different levels while preventing the tree from being overly complex. Hence, the size of the tree is meant to be a measure of the tree's\n",
    "    complexity, hence the name cost-complexity pruning.\n",
    "\n",
    "    One can use cross-validation to pick out an appropriate weight.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from graphviz import Digraph\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class Pruner:\n",
    "    '''\n",
    "    Class for doing pruning of a sci-kit learn DecisionTreeClassifier. At initialization, the order of the nodes to prune is found, but no \n",
    "    pruning is done. The order of pruning is determined by the pruning that results in the smallest increase in the cost (e.g. entropy or gini index)\n",
    "    of the tree is done first. Note, a node can only be pruned if both of its children are leaf nodes (also recall that for a sci-kit learn \n",
    "    DecisionTreeClassifier the children always come in pairs).\n",
    "\n",
    "    Members\n",
    "    -------\n",
    "    tree : sklearn.tree.Tree\n",
    "        A reference to the tree object in a sci-kit learn DecisionTreeClassifier; in such a classifier, this member object is usually called tree_.\n",
    "\n",
    "    leaves : List of Int \n",
    "        A list of the indices which are leaf nodes for the original decision tree.\n",
    "\n",
    "    parents : Numpy array of Int \n",
    "        Provides the index of the parent node for nodei. Note, the fact that the root has no parent is indicated by setting\n",
    "        parents[0] = -1.        \n",
    "    \n",
    "    pruneCosts : Numpy array of Float\n",
    "        The cost increase if nodei is pruned. Note that cost is calculated by weighting a node's impurity (e.g. entropy or gini index) by\n",
    "        the number of samples in the node.\n",
    "\n",
    "    originalCost : Float\n",
    "        The original cost for the fully grown tree, i.e. the total cost for all of the original leaf nodes.\n",
    "\n",
    "    originalChildren : Pair of Numpy array of Int\n",
    "        A copy of the original left and right children indices for the original tree. So it is (children_left.copy(), children_right.copy()).\n",
    "\n",
    "    pruned : Numpy array of Bool\n",
    "        Used for calculating the prune sequence. Holds whether nodei has been pruned. The leaf nodes are considered to automatically have\n",
    "        been pruned.\n",
    "\n",
    "    pruneSequence : Numpy array of Int\n",
    "        The order to prune the nodes. pruneSequence[0] = -1 to indicate the sequence starts with no pruning; so pruneSequence[i] is the ith node\n",
    "        to prune. \n",
    "\n",
    "    pruneState : Int\n",
    "        Holds the current number of nodes pruned. So a state of 0 means no pruning has occurred. This is changed with the member function prune().\n",
    "        Initialized to 0. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, tree):\n",
    "\n",
    "        '''\n",
    "        Finds the prune sequence and initializes the prune state to be 0.\n",
    "\n",
    "        Parameters\n",
    "        ---------\n",
    "        tree : sklearn.tree.Tree\n",
    "            A reference to the tree that will be pruned by this pruner. Note that for sklearn.tree.DecisionTreeClassifier, the tree\n",
    "            is the member variable DecisionTreeClassifier.tree_.\n",
    "        '''\n",
    "\n",
    "        self.tree = tree\n",
    "\n",
    "        self.leaves = self._getLeaves(tree)\n",
    "        self.parents = self._getParents(tree)\n",
    "        self.pruneCosts = self._getPruneCosts(tree)\n",
    "        self.originalCost = self.pruneCosts[self.leaves].sum()\n",
    "        self.originalChildren = list(zip(tree.children_left.copy(), tree.children_right.copy()))\n",
    "\n",
    "        # Initially, only the leaves count as being already pruned.\n",
    "\n",
    "        self.pruned = np.full(len(tree.impurity), False)\n",
    "        self.pruned[self.leaves] = True\n",
    "\n",
    "        self.pruneSequence, self.costSequence = self._makePruneSequence(tree)\n",
    "        self.pruneState = 0\n",
    "\n",
    "    def _getLeaves(self, tree):\n",
    "\n",
    "        '''\n",
    "        Find the leaf nodes of the tree.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            The tree to find the leaf nodes of.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of Int\n",
    "            The list of indices that correspond to leaf nodes in the tree.\n",
    "        '''\n",
    "\n",
    "        leaves = []\n",
    "\n",
    "        # Note that children always come in pairs.\n",
    "\n",
    "        for nodei, lChild in enumerate(tree.children_left):\n",
    "\n",
    "           if lChild == -1:\n",
    "                leaves.append(nodei) \n",
    "       \n",
    "        return leaves \n",
    "\n",
    "    def _getParents(self, tree):\n",
    "        '''\n",
    "        Find the list of indices of parents for each node. The parent of the root node is defined to be -1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            Tree to find the list of parents for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Numpy array of Int\n",
    "            The indices of the parent node for each node in the tree. We consider the parent of the root to be -1.\n",
    "        '''\n",
    "\n",
    "        parents = np.full(len(tree.children_left), -1) \n",
    "\n",
    "        for nodei, children in enumerate(zip(tree.children_left, tree.children_right)):\n",
    "\n",
    "            lChild, rChild = children\n",
    "\n",
    "            # Children always come in pairs for a decision tree.\n",
    "\n",
    "            if lChild != -1:\n",
    "               parents[lChild] = nodei\n",
    "               parents[rChild] = nodei\n",
    "\n",
    "        return parents\n",
    "\n",
    "    def _getCost(self, tree, nodei):\n",
    "\n",
    "        '''\n",
    "        Get the cost of a node; i.e. the product of the impurity and the number of samples. Note, this is not\n",
    "        the cost of pruning the node.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            The tree that the node is in.\n",
    "        nodei : Int\n",
    "            The index of the node to calculate the cost of.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Float\n",
    "            The cost of the node (NOT the cost of pruning the node).\n",
    "        '''\n",
    "\n",
    "        cost = tree.n_node_samples[nodei] * tree.impurity[nodei]\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def _getPruneCosts(self, tree):\n",
    "        '''\n",
    "        Calculate the cost of pruning each node. This is the amount that the total cost of the current pruned\n",
    "        tree will increase by if we prune the node. Is given by the difference between the cost of this node and\n",
    "        the costs of its children.\n",
    "\n",
    "        Note, there isn't really any cost associated with pruning a leaf node as they aren't prunable; so they are given a\n",
    "        cost of 0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            The original unpruned tree to calculate the costs for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Numpy array of Float\n",
    "            The costs of pruning each node in the tree.\n",
    "        '''\n",
    "\n",
    "        pruneCosts = np.zeros(len(tree.impurity))\n",
    "        nodeCosts = tree.n_node_samples * tree.impurity\n",
    "\n",
    "        for nodei, (lChild, rChild) in enumerate( zip( tree.children_left, tree.children_right) ):\n",
    "\n",
    "            # Children always come in pairs.\n",
    "\n",
    "            if lChild != -1:\n",
    "\n",
    "                decrease = nodeCosts[nodei] - nodeCosts[lChild] - nodeCosts[rChild] \n",
    "                pruneCosts[nodei] = decrease\n",
    "\n",
    "        return pruneCosts\n",
    "\n",
    "    def _getInitialCandidates(self, tree):\n",
    "        '''\n",
    "        Find the initial list of prunable nodes (i.e. parents whose both left and right children are leaf nodes).\n",
    "        Also find their prune costs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            The original unpruned tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of Int\n",
    "           The indices of the initial candidates to prune. \n",
    "        List of Float\n",
    "           Their corresponding list of prune costs.\n",
    "        '''\n",
    "\n",
    "        candidates = []\n",
    "        candidateCosts = []\n",
    "\n",
    "        for leafi in self.leaves:\n",
    "\n",
    "            parenti = self.parents[leafi]\n",
    "            if parenti != -1:\n",
    "                lChild = tree.children_left[parenti]\n",
    "                rChild = tree.children_right[parenti]\n",
    "\n",
    "                if self.pruned[lChild] and self.pruned[rChild] and parenti not in candidates:\n",
    "                    candidates.append(parenti)\n",
    "                    candidateCosts.append(self.pruneCosts[parenti])\n",
    "\n",
    "        return candidates, candidateCosts\n",
    "\n",
    "    def _popNextPrune(self, candidates, costs):\n",
    "        '''\n",
    "        Remove the next prune node from the list of candidates, and also remove its cost from the list of costs. \n",
    "\n",
    "        The next node to prune is found by minimizing over the costs of all of the candidates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        candidates : List of Int\n",
    "            The list of indices of nodes that we could potentially prune next.\n",
    "        costs : List of Float\n",
    "            The corresponding list of pruning costs for each candidate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Int\n",
    "            The index of the next prune node.\n",
    "        '''\n",
    "\n",
    "        minCosti = np.argmin(costs)\n",
    "        nextPrune = candidates.pop(minCosti)\n",
    "        costs.pop(minCosti)\n",
    "\n",
    "        return nextPrune\n",
    "        \n",
    "\n",
    "    def _makePruneSequence(self, tree):\n",
    "        '''\n",
    "        Find the order to prune the nodes for cost-complexity pruning. The order is determined by the fact that nodes with the smallest\n",
    "        pruning cost are pruned first. Also find the accumulative pruning cost for pruning in this order.\n",
    "\n",
    "        Note that pruneSequence[0] = -1, indicating the no pruning. Also costSequence[0] = 0 as no pruning has occured. \n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : sklearn.tree.Tree\n",
    "            The original unpruned tree.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Numpy array of Int\n",
    "            The order to prune the nodes.\n",
    "\n",
    "        Numpy array of Float\n",
    "            The total accumulative pruning cost for pruning the nodes in order.\n",
    "        '''\n",
    "\n",
    "        pruneSequence = [-1]\n",
    "        costSequence = [0]\n",
    "        currentCost = 0\n",
    "\n",
    "        candidates, costs = self._getInitialCandidates(tree)\n",
    "\n",
    "        while candidates:\n",
    "\n",
    "            prunei = self._popNextPrune(candidates, costs)\n",
    "            self.pruned[prunei] = True\n",
    "            pruneSequence.append(prunei)\n",
    "            currentCost += self.pruneCosts[prunei]\n",
    "            costSequence.append(currentCost)\n",
    "\n",
    "            parenti = self.parents[prunei]\n",
    "            if parenti != -1:\n",
    "                lChild = tree.children_left[parenti]\n",
    "                rChild = tree.children_right[parenti]\n",
    "\n",
    "                if self.pruned[lChild] and self.pruned[rChild]:\n",
    "                    candidates.append(parenti)\n",
    "                    costs.append(self.pruneCosts[parenti])\n",
    "\n",
    "        return np.array(pruneSequence), np.array(costSequence)\n",
    "\n",
    "    def prune(self, prunei):\n",
    "        '''\n",
    "        Do pruning/unpruning on the tree. Technically, pruning is done on splits (and not on nodes).\n",
    "        We specify the number of split to prune away from the ORIGINAL tree.\n",
    "\n",
    "        If the number of splits to prune is greater than what we have pruned so far, we prune off\n",
    "        more splits. If it is less, then we unprune (i.e. restore) splits.\n",
    "        Parameters\n",
    "        ----------\n",
    "        prunei : Int\n",
    "            The number of splits to prune off the original tree. Negative values specify offset\n",
    "            from the maximum number of prunes possible, similar to how negative indexing of\n",
    "            arrays works. \n",
    "\n",
    "        '''\n",
    "\n",
    "        nPrunes = len(self.pruneSequence)\n",
    "\n",
    "        if prunei < 0:\n",
    "            prunei += nPrunes  \n",
    "\n",
    "        # If the new state involves more prunes than the old state, we have to prune nodes.\n",
    "        # Else we need to restore children to their old state.\n",
    "\n",
    "        if prunei > self.pruneState:\n",
    "\n",
    "            for prune in range(self.pruneState + 1, prunei + 1):\n",
    "                nodei = self.pruneSequence[prune]\n",
    "                self.tree.children_left[nodei] = -1\n",
    "                self.tree.children_right[nodei] = -1\n",
    "\n",
    "        elif prunei < self.pruneState: \n",
    "\n",
    "            for prune in range(prunei + 1, self.pruneState + 1):\n",
    "                nodei = self.pruneSequence[prune]\n",
    "                lChild, rChild = self.originalChildren[nodei]\n",
    "                self.tree.children_left[nodei] = lChild\n",
    "                self.tree.children_right[nodei] = rChild\n",
    "\n",
    "        # Update the prune state.\n",
    "\n",
    "        self.pruneState = prunei\n",
    "\n",
    "    def costComplexity(self, complexityWeight):\n",
    "        '''\n",
    "        Compute the cost-complexity curve for a given weight of the complexity. The complexity is simply the number\n",
    "        of nodes in the pruned tree. So the cost-complexity is a combination of the cost of the tree and the weighted\n",
    "        size. To the find the optimal complexity weight, one can do something such as cross-validation.\n",
    "        \n",
    "        Also, return a list of the sizes for each cost-complexity.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        complexityWeight : Float         \n",
    "            The weight to apply to the complexity measure.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Numpy of Int\n",
    "            The size of the pruned tree for each cost-complexity.\n",
    "\n",
    "        Numpy of Float\n",
    "            The cost-complexity measure for each tree size.\n",
    "        '''\n",
    "\n",
    "        nPrunes = len(self.pruneSequence)\n",
    "\n",
    "        # Recall that each prune removes two nodes.\n",
    "        sizes = self.tree.node_count - 2 * np.arange(0, nPrunes)\n",
    "\n",
    "        costs = np.full(len(sizes), self.originalCost)\n",
    "        costs += self.costSequence \n",
    "        costComplexity = costs + complexityWeight * sizes\n",
    "\n",
    "        return sizes, costComplexity\n",
    "\n",
    "    def pruneForCostComplexity(self, complexityWeight):\n",
    "        '''\n",
    "        Prune the tree to the minimal cost-complexity for the given provided weight.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        complexityWeight : Float\n",
    "            The complexity weight to use for calculating cost-complexity.\n",
    "        '''\n",
    "\n",
    "        sizes, costComplexity = self.costComplexity(complexityWeight)\n",
    "\n",
    "        minI = np.argmin(costComplexity)\n",
    "\n",
    "        self.prune(minI)\n",
    "\n",
    "class Box:\n",
    "    '''\n",
    "    Class to keep track of the xy-rectangle that a node in a decision tree classifier applies to.\n",
    "\n",
    "    Can be used to accurately and precisely draw the output of a decision tree classifier.\n",
    "\n",
    "    Members\n",
    "    -------\n",
    "    lowerBounds : List of Float of size 2.\n",
    "        Holds the lower bounds of the x and y coordinates.\n",
    "\n",
    "    upperBounds : List of Float of size 2.\n",
    "        Holds the upper bounds of the x and y coordinates.\n",
    "\n",
    "    value : None or Int\n",
    "        If desired, one can specify the value that the tree is supposed to resolve the node to. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, lowerBounds, upperBounds, value = None): \n",
    "        '''\n",
    "        Initialize the member variables.\n",
    "        Parameters\n",
    "        ----------\n",
    "        lowerBounds : List of Float of size 2\n",
    "            Holds the lower bounds of the x and y coordinates.\n",
    "\n",
    "        upperBounds : List of Float of size 2\n",
    "            Holds the upper bounds of the x and y coordinates.\n",
    "\n",
    "        value : The value of a node that the box represents. Default is None. \n",
    "        '''\n",
    "\n",
    "        self.lowerBounds = lowerBounds\n",
    "        self.upperBounds = upperBounds\n",
    "        self.value = value\n",
    "\n",
    "    def split(self, feature, value):\n",
    "        '''\n",
    "        Split the box into two boxes specified by whether x or y coordinate (i.e. feature) and the threshold value to split at.\n",
    "        This corresponds to how a node in a classifying tree splits on a feature and threshold value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature : Int\n",
    "            0 for x-coordinate or 1 for y-coordinate.\n",
    "\n",
    "        value : Float\n",
    "            The threshold value to do the split. The left box is feature less than or equal to value. The right box is the feature\n",
    "            greater than this value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Box\n",
    "            This is the left box corresponding to the left child of a split node in a decision tree classifier.\n",
    "\n",
    "        Box\n",
    "            This is the right box corresponding to the right child of a split node in a decision tree classifier. \n",
    "        '''\n",
    "\n",
    "        newUpper = self.upperBounds.copy()\n",
    "        newUpper[feature] = value\n",
    "\n",
    "        newLower = self.lowerBounds.copy()\n",
    "        newLower[feature] = value\n",
    "\n",
    "        return Box(self.lowerBounds, newUpper), Box(newLower, self.upperBounds)\n",
    "\n",
    "    def _color(self):\n",
    "        '''\n",
    "        Get the color for this box using a colormap.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Color \n",
    "            The colormap output. \n",
    "        '''\n",
    "\n",
    "        cmap = cm.get_cmap(\"winter\")\n",
    "\n",
    "        if self.value == None:\n",
    "            return cmap(0.0)\n",
    "\n",
    "        return cmap(float(self.value))\n",
    "\n",
    "    def convertMatPlotLib(self, edges = False):\n",
    "        '''\n",
    "        Convert the box a matplotlib.patches.Rectangle.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edges : Bool\n",
    "            Whether to include edges in the rectangle drawing. Default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        matplotlib.patches.Rectangle\n",
    "            The matplotlib patch for this box.\n",
    "        '''\n",
    "\n",
    "        width = self.upperBounds[0] - self.lowerBounds[0]\n",
    "        height = self.upperBounds[1] - self.lowerBounds[1]\n",
    "\n",
    "        kwargs = {'xy' : (self.lowerBounds[0], self.lowerBounds[1]),\n",
    "                  'width' : width,\n",
    "                  'height' : height,\n",
    "                 } \n",
    "        if edges:\n",
    "            kwargs['facecolor'] = self._color()\n",
    "        else:\n",
    "            kwargs['color'] = self._color()\n",
    "\n",
    "        return Rectangle(**kwargs)\n",
    "\n",
    "        \n",
    "def getLeafBoxes(tree, lowerBounds, upperBounds):\n",
    "    ''' \n",
    "    Get a list of Box for the leaf nodes of a tree and the initial bounds on the output.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    tree : sklearn.tree.Tree\n",
    "        The tree that determines how to split the boxes to find the boxes corresponding to the\n",
    "        leaf nodes.\n",
    "    lowerBounds : List of Float of Size 2\n",
    "        The initial lower bounds on the xy-coordinates of the box.\n",
    "\n",
    "    upperBounds : List of Float of Size 2\n",
    "        The inital upper bounds on the xy-coorinates of the box.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of Box\n",
    "        A list of Box for each leaf node in the tree.\n",
    "    '''\n",
    "\n",
    "    rootBox = Box(lowerBounds, upperBounds)\n",
    "\n",
    "    boxes = [(0, rootBox)]\n",
    "    leaves = []\n",
    "\n",
    "    # Keep splitting boxes until we are at the leaf level. Use the thresholds and features contained in\n",
    "    # the tree to do the splitting.\n",
    "\n",
    "    while boxes:\n",
    "\n",
    "        nodei, box = boxes.pop()\n",
    "        lChild = tree.children_left[nodei]\n",
    "\n",
    "        # If there are no children then we are at a leaf; recall that children always come in pairs for a decision\n",
    "        # tree.\n",
    "\n",
    "        if lChild == -1:\n",
    "\n",
    "            box.value = np.argmax(tree.value[nodei])\n",
    "            leaves.append(box)\n",
    "\n",
    "        else:\n",
    "\n",
    "            rChild = tree.children_right[nodei]\n",
    "\n",
    "            lBox, rBox = box.split(tree.feature[nodei], tree.threshold[nodei]) \n",
    "            boxes.append((lChild, lBox))\n",
    "            boxes.append((rChild, rBox))\n",
    "\n",
    "    return leaves\n",
    "\n",
    "def plotTreeOutput(axis, tree, lowerBounds, upperBounds, edges = False):\n",
    "    '''\n",
    "    Get a precise and accurate plot of the output of a decision tree inside a given box.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    axis : pyplot axis\n",
    "        The axis to plot to.\n",
    "\n",
    "    tree : sklearn.tree.Tree\n",
    "        The tree to plot.\n",
    "\n",
    "    lowerBounds : List of Float of Size 2\n",
    "        The lower bounds of the xy-coordinates of the box to graph.\n",
    "\n",
    "    upperBounds : List of Float of Size 2\n",
    "        The upper bounds of the xy-coordinates of the box to graph.\n",
    "\n",
    "    edges : Bool\n",
    "        Whether to include edges of the leaf boxes in the output. Default is False.\n",
    "    '''\n",
    "\n",
    "    # The output is determined by the leaf nodes of the decision tree.\n",
    "\n",
    "    leafBoxes = getLeafBoxes(tree, lowerBounds, upperBounds) \n",
    "    \n",
    "    for box in leafBoxes:\n",
    "    \n",
    "        rect = box.convertMatPlotLib(edges)\n",
    "        axis.add_patch(rect)\n",
    "\n",
    "def makeSimpleGraphViz(tree):\n",
    "    ''' \n",
    "    Make a simple graphviz vizualization for a decision tree. For each node, we simply\n",
    "    output the index of the node in the tree, and if the node isn't a leaf, then its\n",
    "    pruning cost.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : sklearn.tree.Tree\n",
    "        The tree to visualize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Digraph\n",
    "        Reference to the graphviz object created.\n",
    "    '''\n",
    "\n",
    "    g = Digraph('g')\n",
    "\n",
    "    # Make nodes\n",
    "\n",
    "    nodes = [0]\n",
    "\n",
    "    while nodes:\n",
    "\n",
    "        node = nodes.pop()\n",
    "\n",
    "        lChild = tree.children_left[node]\n",
    "        rChild = tree.children_right[node]\n",
    "\n",
    "        # Non-leaf nodes contain information on the cost of pruning away their split.\n",
    "\n",
    "        if lChild != -1:\n",
    "            costDecrease = tree.impurity[node] * tree.n_node_samples[node]\n",
    "            costDecrease -= tree.impurity[lChild] * tree.n_node_samples[lChild]\n",
    "            costDecrease -= tree.impurity[rChild] * tree.n_node_samples[rChild] \n",
    "            costDecrease = \"\\n\" + (\"% .1f\" % costDecrease) \n",
    "\n",
    "            nodes.append(lChild)\n",
    "            nodes.append(rChild) \n",
    "        else:\n",
    "            costDecrease = \"\"\n",
    "        g.node(str(node), str(node) + costDecrease) \n",
    "\n",
    "    # Make edges\n",
    "\n",
    "    for node, children in enumerate(zip(tree.children_left, tree.children_right)):\n",
    "\n",
    "        lchild, rchild = children \n",
    "\n",
    "        if lchild != -1:\n",
    "            g.edge(str(node), str(lchild))\n",
    "\n",
    "        if rchild != -1:\n",
    "            g.edge(str(node), str(rchild))\n",
    "\n",
    "    return g\n",
    "\n",
    "def makePrunerGraphViz(pruner):\n",
    "    '''\n",
    "    Make a simple graphviz vizualization of a pruner's state of pruning a decision tree. For each node, \n",
    "    we simply output its index and its prune cost (if it isn't a leaf node of the original tree). Also,\n",
    "    we highlight active nodes in green, and unactive nodes (i.e. pruned away) in red.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pruner : Pruner\n",
    "        The pruner attached to the pruned tree that we wish to visualize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Digraph\n",
    "        Reference to the graphviz object for the visualization.\n",
    "    '''\n",
    "\n",
    "    tree = pruner.tree\n",
    "    g = Digraph('g') #, filename = filename)\n",
    "\n",
    "    # Make nodes\n",
    "\n",
    "    for node, ((lChild, rChild), newChild, parent) in enumerate(zip(pruner.originalChildren, tree.children_left, pruner.parents)):\n",
    "\n",
    "        # The root node never has a pruned parent.\n",
    "\n",
    "        if parent != -1:\n",
    "            parentPruned = tree.children_left[parent] == -1\n",
    "        else:\n",
    "            parentPruned = False\n",
    "\n",
    "        nodePruned = newChild == -1\n",
    "\n",
    "        # Non-leaf nodes (in the original tree) contain information on the cost of pruning away their split.\n",
    "\n",
    "        if lChild != -1:\n",
    "            costDecrease = tree.impurity[node] * tree.n_node_samples[node]\n",
    "            costDecrease -= tree.impurity[lChild] * tree.n_node_samples[lChild]\n",
    "            costDecrease -= tree.impurity[rChild] * tree.n_node_samples[rChild] \n",
    "            costDecrease = \"\\n\" + (\"% .1f\" % costDecrease) \n",
    "        else:\n",
    "            costDecrease = \"\"\n",
    "\n",
    "        # Active nodes are green and non-active nodes are red. Non-active includes nodes that have\n",
    "        # been pruned, but are still leaves in the prune tree.\n",
    "\n",
    "        if parentPruned:\n",
    "            g.node(str(node), str(node) + costDecrease, color = \"red\", style = 'filled')\n",
    "   \n",
    "        else:\n",
    "            g.node(str(node), str(node) + costDecrease, color = \"green\", style = 'filled')\n",
    "\n",
    "    # Make edges\n",
    "\n",
    "    for node, children in enumerate(pruner.originalChildren):\n",
    "\n",
    "        lchild, rchild = children \n",
    "\n",
    "        if lchild != -1:\n",
    "            g.edge(str(node), str(lchild))\n",
    "\n",
    "        if rchild != -1:\n",
    "            g.edge(str(node), str(rchild))\n",
    "\n",
    "    return g\n",
    "\n",
    "def doCrossValidation(model, x, y, nCrossVal, weights):\n",
    "    '''\n",
    "    Do cross validation for different complexity weights. Use the results to determine\n",
    "    the best weight to use. For each weight, this finds the optimal pruning that\n",
    "    minimizes the cost-complexity for the given complexity weight.\n",
    "\n",
    "    Paramters\n",
    "    ---------\n",
    "    model : sklearn.tree.DecisionTreeClassifier\n",
    "        The tree model to use.\n",
    "\n",
    "    x : Numpy Array of Shape (nPoints, 2)\n",
    "        The dependent variables, i.e. features.\n",
    "\n",
    "    y : Numpy Array of Shape (nPoints, 1)\n",
    "        The target class for each data point.\n",
    "\n",
    "    nCrossVal : Int\n",
    "        The number of cross validations to do for each weight.\n",
    "\n",
    "    weights : Numpy array of Float\n",
    "        The different values of the complexity weights to do cross validation over.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Numpy Array of Int of Shape (nCrossVal, len(weights))\n",
    "        The sizes of the optimal trees for each run.\n",
    "    Numpy Array of Float of Shape (nCrossVal, len(weights))\n",
    "        The accuracy score of the optimal tree for each run. \n",
    "    '''\n",
    "\n",
    "    scores = []\n",
    "    sizes = []\n",
    "\n",
    "    # For each repetition of cross-validation, we iterate over all weights.\n",
    "\n",
    "    for i in range(nCrossVal):\n",
    "    \n",
    "        xtrain, xtest, ytrain, ytest = train_test_split(x, y)\n",
    "        model.fit(xtrain, ytrain)\n",
    "        pruner = Pruner(model.tree_)\n",
    "\n",
    "        # Find the optimal pruning for each weight.\n",
    "    \n",
    "        runScores = []\n",
    "        runSizes = []\n",
    "\n",
    "        for weight in weights:\n",
    "            \n",
    "            treeSizes, costComplexity = pruner.costComplexity(weight)\n",
    "            minI = np.argmin(costComplexity)\n",
    "            pruner.prune(minI)\n",
    "            ypredict = model.predict(xtest)\n",
    "            acc = accuracy_score(ytest, ypredict)\n",
    "    \n",
    "            runScores.append(acc)\n",
    "            runSizes.append(treeSizes[minI])\n",
    "    \n",
    "        scores.append(runScores)\n",
    "        sizes.append(runSizes)\n",
    "    \n",
    "    scores = np.array(scores) \n",
    "    sizes = np.array(sizes)\n",
    "   \n",
    "    return sizes, scores "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
